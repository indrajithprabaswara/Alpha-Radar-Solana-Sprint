{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Radar Solana Sprint \u2014 High-Recall CatBoost Solution\n",
    "\n",
    "This notebook rebuilds the end-to-end pipeline for the Alpha Radar Solana Sprint competition.  It focuses on\n",
    "feature engineering that captures ultra-early token behaviours, robust cross-validation with F1-driven threshold\n",
    "tuning, and a final CatBoost model that is optimised for recall without sacrificing precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment and configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    jaccard_score,\n",
    "    precision_recall_curve,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(2025)\n",
    "\n",
    "BASE_DIR = Path('.')\n",
    "DATA_DIR = BASE_DIR / 'Dataset' / 'alpha-radar-solana-sprint'\n",
    "TARGET_PATH = BASE_DIR / 'Dataset' / 'target_tokens.csv'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "EXPECTED_EVAL_ROWS = 64208\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timestamp_to_seconds(value) -> float:\n",
    "    'Convert a mm:ss.s style timestamp string into float seconds.'\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return np.nan\n",
    "    if ':' not in text:\n",
    "        try:\n",
    "            return float(text)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    try:\n",
    "        minutes, seconds = text.split(':', 1)\n",
    "        return int(minutes) * 60.0 + float(seconds)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    'Soft-convert numeric looking object columns.'\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        sample = df[col].dropna().astype(str)\n",
    "        if sample.empty:\n",
    "            continue\n",
    "        sample = sample.str.replace(',', '', regex=False)\n",
    "        numeric_ratio = sample.str.fullmatch(r'-?\\d+(?:\\.\\d+)?').mean()\n",
    "        if numeric_ratio >= 0.85:\n",
    "            df[col] = pd.to_numeric(sample, errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_event_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if 'index' in df.columns:\n",
    "        df = df.drop(columns=['index'])\n",
    "    if 'timestamp' in df.columns and 'timestamp_seconds' not in df.columns:\n",
    "        df['timestamp_seconds'] = df['timestamp'].apply(parse_timestamp_to_seconds).astype('float32')\n",
    "    df = coerce_numeric(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_events(paths: Iterable[Path]) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for path in paths:\n",
    "        print(f'Loading {path.name} ...')\n",
    "        frame = load_event_csv(path)\n",
    "        frames.append(frame)\n",
    "    data = pd.concat(frames, ignore_index=True)\n",
    "    print(f'Loaded {len(data):,} rows across {len(frames)} file(s).')\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_target_tokens(path: Path) -> pd.Index:\n",
    "    target_df = pd.read_csv(path)\n",
    "    if target_df.shape[1] == 1:\n",
    "        series = target_df.iloc[:, 0]\n",
    "    else:\n",
    "        for candidate in ['mint_token_id', 'token', 'mint']:\n",
    "            if candidate in target_df.columns:\n",
    "                series = target_df[candidate]\n",
    "                break\n",
    "        else:\n",
    "            series = target_df.iloc[:, 0]\n",
    "    tokens = series.dropna().astype(str).unique()\n",
    "    return pd.Index(tokens, name='mint_token_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGG_NUMERIC_FUNCS = ['mean', 'std', 'min', 'max', 'sum', 'median', 'last']\n",
    "\n",
    "\n",
    "def aggregate_numeric(grouped: pd.core.groupby.generic.DataFrameGroupBy, numeric_cols: List[str]) -> pd.DataFrame:\n",
    "    if not numeric_cols:\n",
    "        return pd.DataFrame(index=grouped.size().index)\n",
    "    agg = grouped[numeric_cols].agg(AGG_NUMERIC_FUNCS)\n",
    "    agg.columns = [f'{col}_{stat}' for col, stat in agg.columns]\n",
    "    return agg\n",
    "\n",
    "\n",
    "def pivot_categorical(grouped: pd.core.groupby.generic.DataFrameGroupBy, cat_cols: List[str], prefix: str) -> pd.DataFrame:\n",
    "    pivots = []\n",
    "    for col in cat_cols:\n",
    "        counts = grouped[col].value_counts(normalize=False).unstack(fill_value=0)\n",
    "        counts.columns = [f'{prefix}{col}_{str(c)}_count' for c in counts.columns]\n",
    "        ratios = counts.div(counts.sum(axis=1).replace(0, np.nan), axis=0)\n",
    "        ratios = ratios.add_suffix('_ratio').fillna(0)\n",
    "        pivots.append(counts)\n",
    "        pivots.append(ratios)\n",
    "    if not pivots:\n",
    "        return pd.DataFrame(index=grouped.size().index)\n",
    "    out = pd.concat(pivots, axis=1)\n",
    "    out.columns = pd.Index(out.columns).map(str)\n",
    "    return out\n",
    "\n",
    "\n",
    "def derive_first_n_features(ordered: pd.DataFrame, grouped: pd.core.groupby.generic.DataFrameGroupBy, n: int) -> pd.DataFrame:\n",
    "    first_n = ordered.groupby('mint_token_id', sort=False).head(n)\n",
    "    feature_frames = []\n",
    "    if 'timestamp_seconds' in first_n.columns:\n",
    "        timing = first_n.groupby('mint_token_id')['timestamp_seconds'].agg(['min', 'max', 'mean']).rename(columns={\n",
    "            'min': f'first{n}_time_min',\n",
    "            'max': f'first{n}_time_max',\n",
    "            'mean': f'first{n}_time_mean'\n",
    "        })\n",
    "        feature_frames.append(timing)\n",
    "    if 'event_type' in first_n.columns:\n",
    "        counts = first_n.groupby(['mint_token_id', 'event_type']).size().unstack(fill_value=0)\n",
    "        counts.columns = [f'first{n}_event_{c}_count' for c in counts.columns]\n",
    "        feature_frames.append(counts)\n",
    "    numeric_candidates = [\n",
    "        col for col in first_n.columns\n",
    "        if col not in {'mint_token_id'} and pd.api.types.is_numeric_dtype(first_n[col])\n",
    "    ]\n",
    "    if numeric_candidates:\n",
    "        agg = first_n.groupby('mint_token_id')[numeric_candidates].agg(['mean', 'max', 'min']).astype('float32')\n",
    "        agg.columns = [f'first{n}_{col}_{stat}' for col, stat in agg.columns]\n",
    "        feature_frames.append(agg)\n",
    "    if feature_frames:\n",
    "        return pd.concat(feature_frames, axis=1)\n",
    "    return pd.DataFrame(index=grouped.size().index)\n",
    "\n",
    "\n",
    "def build_token_features(events: pd.DataFrame, *, top_categories: int = 10, first_n_events: Tuple[int, ...] = (5, 10, 20)) -> pd.DataFrame:\n",
    "    if 'mint_token_id' not in events.columns:\n",
    "        raise KeyError('Expected mint_token_id column to group events by token.')\n",
    "\n",
    "    events = events.copy()\n",
    "    events['mint_token_id'] = events['mint_token_id'].astype(str)\n",
    "\n",
    "    if 'timestamp_seconds' not in events.columns and 'timestamp' in events.columns:\n",
    "        events['timestamp_seconds'] = events['timestamp'].apply(parse_timestamp_to_seconds).astype('float32')\n",
    "\n",
    "    ordered = events.sort_values(['mint_token_id', 'timestamp_seconds'], kind='mergesort')\n",
    "    grouped = ordered.groupby('mint_token_id', sort=False)\n",
    "\n",
    "    numeric_cols = [\n",
    "        col for col in ordered.columns\n",
    "        if col not in {'mint_token_id'} and pd.api.types.is_numeric_dtype(ordered[col])\n",
    "    ]\n",
    "    categorical_cols = [\n",
    "        col for col in ordered.columns\n",
    "        if ordered[col].dtype == 'object' and col not in {'mint_token_id'}\n",
    "    ]\n",
    "\n",
    "    base = pd.DataFrame(index=grouped.size().index)\n",
    "    base['event_count'] = grouped.size().astype('int32')\n",
    "    if 'timestamp_seconds' in ordered.columns:\n",
    "        base['lifetime_seconds'] = (grouped['timestamp_seconds'].max() - grouped['timestamp_seconds'].min()).fillna(0)\n",
    "        base['time_to_first_trade'] = grouped['timestamp_seconds'].min().fillna(0)\n",
    "\n",
    "    if 'holder' in ordered.columns:\n",
    "        base['unique_holders'] = grouped['holder'].nunique(dropna=True)\n",
    "\n",
    "    if 'trade_signature' in ordered.columns:\n",
    "        base['unique_trades'] = grouped['trade_signature'].nunique(dropna=True)\n",
    "\n",
    "    numeric_summary = aggregate_numeric(grouped, numeric_cols)\n",
    "\n",
    "    pivot_frames = []\n",
    "    if categorical_cols:\n",
    "        capped_cols = {}\n",
    "        for col in categorical_cols:\n",
    "            top_values = ordered[col].value_counts().head(top_categories).index\n",
    "            capped_cols[col] = ordered[col].where(ordered[col].isin(top_values), other='__OTHER__')\n",
    "        capped = ordered.assign(**{col: capped_cols[col] for col in capped_cols})\n",
    "        capped_grouped = capped.groupby('mint_token_id', sort=False)\n",
    "        pivot_frames.append(pivot_categorical(capped_grouped, list(capped_cols.keys()), prefix='cat_'))\n",
    "\n",
    "    first_n_frames = [derive_first_n_features(ordered, grouped, n) for n in first_n_events]\n",
    "\n",
    "    features = pd.concat([base, numeric_summary, *pivot_frames, *first_n_frames], axis=1).fillna(0)\n",
    "\n",
    "    for col in features.columns:\n",
    "        if pd.api.types.is_float_dtype(features[col]):\n",
    "            features[col] = features[col].astype('float32')\n",
    "        elif pd.api.types.is_integer_dtype(features[col]):\n",
    "            features[col] = features[col].astype('int32')\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the training matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sources = [DATA_DIR / 'Sample_Dataset.csv']\n",
    "train_events = load_events(training_sources)\n",
    "train_features = build_token_features(train_events)\n",
    "\n",
    "positive_tokens = load_target_tokens(TARGET_PATH)\n",
    "train_features['is_target'] = train_features.index.isin(positive_tokens).astype('int8')\n",
    "\n",
    "print(f'Training tokens: {len(train_features):,}')\n",
    "print(train_features['is_target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stratified cross-validation with F1-driven threshold search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    fold: int\n",
    "    best_threshold: float\n",
    "    auc: float\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "    jaccard: float\n",
    "\n",
    "\n",
    "def find_best_threshold(y_true: np.ndarray, y_prob: np.ndarray) -> Tuple[float, Dict[str, float]]:\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    thresholds = np.append(thresholds, 1.0)\n",
    "    f1_scores = 2 * precision * recall / np.maximum(precision + recall, 1e-8)\n",
    "    best_idx = np.nanargmax(f1_scores)\n",
    "    best_thr = float(thresholds[best_idx])\n",
    "    preds = (y_prob >= best_thr).astype(int)\n",
    "    precision_val, recall_val, f1_val, _ = precision_recall_fscore_support(y_true, preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    jac = jaccard_score(y_true, preds, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    metrics = {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision_val,\n",
    "        'recall': recall_val,\n",
    "        'f1': f1_val,\n",
    "        'jaccard': jac,\n",
    "        'auc': auc,\n",
    "    }\n",
    "    return best_thr, metrics\n",
    "\n",
    "\n",
    "features = train_features.drop(columns=['is_target'])\n",
    "labels = train_features['is_target'].astype(int)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2025)\n",
    "fold_results: List[FoldResult] = []\n",
    "validation_rows = []\n",
    "\n",
    "class_weight = max(1.0, (len(labels) - labels.sum()) / max(labels.sum(), 1))\n",
    "\n",
    "params = dict(\n",
    "    iterations=2500,\n",
    "    learning_rate=0.035,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3.0,\n",
    "    random_seed=2025,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    bootstrap_type='Bernoulli',\n",
    "    subsample=0.9,\n",
    "    rsm=0.6,\n",
    "    grow_policy='Lossguide',\n",
    "    min_data_in_leaf=32,\n",
    "    scale_pos_weight=class_weight,\n",
    "    early_stopping_rounds=200,\n",
    "    allow_writing_files=False,\n",
    "    verbose=250,\n",
    "    custom_metric=['F1', 'Recall'],\n",
    ")\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(features, labels), start=1):\n",
    "    X_train, X_valid = features.iloc[train_idx], features.iloc[valid_idx]\n",
    "    y_train, y_valid = labels.iloc[train_idx], labels.iloc[valid_idx]\n",
    "\n",
    "    train_pool = Pool(X_train, y_train)\n",
    "    valid_pool = Pool(X_valid, y_valid)\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
    "\n",
    "    valid_prob = model.predict_proba(X_valid)[:, 1]\n",
    "    best_thr, fold_metrics = find_best_threshold(y_valid.to_numpy(), valid_prob)\n",
    "\n",
    "    fold_results.append(FoldResult(\n",
    "        fold=fold,\n",
    "        best_threshold=best_thr,\n",
    "        auc=fold_metrics['auc'],\n",
    "        accuracy=fold_metrics['accuracy'],\n",
    "        precision=fold_metrics['precision'],\n",
    "        recall=fold_metrics['recall'],\n",
    "        f1=fold_metrics['f1'],\n",
    "        jaccard=fold_metrics['jaccard'],\n",
    "    ))\n",
    "\n",
    "    validation_rows.append(pd.DataFrame({\n",
    "        'fold': fold,\n",
    "        'mint_token_id': features.index[valid_idx],\n",
    "        'y_true': y_valid.to_numpy(),\n",
    "        'y_prob': valid_prob,\n",
    "    }))\n",
    "\n",
    "fold_df = pd.DataFrame([fr.__dict__ for fr in fold_results])\n",
    "fold_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_threshold = float(fold_df['best_threshold'].median())\n",
    "print(f'Median threshold (F1-optimal): {median_threshold:.4f}')\n",
    "print(fold_df[['fold', 'auc', 'accuracy', 'precision', 'recall', 'f1', 'jaccard', 'best_threshold']])\n",
    "\n",
    "stacked_val = pd.concat(validation_rows, ignore_index=True)\n",
    "stacked_val['y_pred'] = (stacked_val['y_prob'] >= median_threshold).astype(int)\n",
    "\n",
    "overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(\n",
    "    stacked_val['y_true'], stacked_val['y_pred'], average='binary', zero_division=0\n",
    ")\n",
    "overall_acc = accuracy_score(stacked_val['y_true'], stacked_val['y_pred'])\n",
    "overall_jaccard = jaccard_score(stacked_val['y_true'], stacked_val['y_pred'], zero_division=0)\n",
    "overall_auc = roc_auc_score(stacked_val['y_true'], stacked_val['y_prob'])\n",
    "\n",
    "print(json.dumps({\n",
    "    'accuracy': round(overall_acc, 6),\n",
    "    'precision': round(overall_precision, 6),\n",
    "    'recall': round(overall_recall, 6),\n",
    "    'f1': round(overall_f1, 6),\n",
    "    'jaccard': round(overall_jaccard, 6),\n",
    "    'auc': round(overall_auc, 6),\n",
    "}, indent=2))\n",
    "\n",
    "conf_mat = confusion_matrix(stacked_val['y_true'], stacked_val['y_pred'])\n",
    "conf_df = pd.DataFrame(conf_mat, index=['Actual 0', 'Actual 1'], columns=['Pred 0', 'Pred 1'])\n",
    "conf_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the final CatBoost model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = params.copy()\n",
    "final_params.update(dict(verbose=250, early_stopping_rounds=150))\n",
    "\n",
    "final_model = CatBoostClassifier(**final_params)\n",
    "full_pool = Pool(features, labels)\n",
    "final_model.fit(full_pool)\n",
    "\n",
    "feature_importance = pd.Series(final_model.get_feature_importance(), index=features.columns)\n",
    "feature_importance.sort_values(ascending=False).head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference on the evaluation split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_paths = sorted(DATA_DIR.glob('evaluation_set_30s_chunk_*.csv'))\n",
    "if not eval_paths:\n",
    "    raise FileNotFoundError('Evaluation chunks were not found. Make sure the updated evaluation set is present.')\n",
    "\n",
    "eval_events = load_events(eval_paths)\n",
    "eval_token_index = pd.Index(\n",
    "    eval_events['mint_token_id'].astype(str).drop_duplicates(),\n",
    "    name='mint_token_id'\n",
    ")\n",
    "if len(eval_token_index) != EXPECTED_EVAL_ROWS:\n",
    "    print(f\"Warning: expected {EXPECTED_EVAL_ROWS:,} evaluation tokens but found {len(eval_token_index):,} in the loaded events.\")\n",
    "\n",
    "eval_features = build_token_features(eval_events)\n",
    "\n",
    "missing_cols = set(features.columns) - set(eval_features.columns)\n",
    "for col in missing_cols:\n",
    "    eval_features[col] = 0.0\n",
    "extra_cols = set(eval_features.columns) - set(features.columns)\n",
    "if extra_cols:\n",
    "    eval_features = eval_features.drop(columns=list(extra_cols))\n",
    "\n",
    "eval_features = eval_features[features.columns]\n",
    "eval_features = eval_features.reindex(eval_token_index).fillna(0.0)\n",
    "for col, dtype in features.dtypes.items():\n",
    "    eval_features[col] = eval_features[col].astype(dtype)\n",
    "\n",
    "if len(eval_features) != EXPECTED_EVAL_ROWS:\n",
    "    raise ValueError(f\"Submission must contain {EXPECTED_EVAL_ROWS:,} rows, but prepared {len(eval_features):,}.\")\n",
    "\n",
    "probabilities = final_model.predict_proba(eval_features)[:, 1]\n",
    "predictions = (probabilities >= median_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'mint_token_id': eval_features.index,\n",
    "    'is_target': predictions,\n",
    "})\n",
    "submission_path = OUTPUT_DIR / 'submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f'Saved submission to {submission_path}')\n",
    "\n",
    "report = pd.DataFrame({\n",
    "    'token': eval_features.index,\n",
    "    'threshold': median_threshold,\n",
    "    'prediction_value': probabilities,\n",
    "    'isTargetToken': predictions,\n",
    "})\n",
    "report_path = OUTPUT_DIR / 'detailed_predictions.csv'\n",
    "report.to_csv(report_path, index=False)\n",
    "print(f'Saved detailed predictions to {report_path}')\n",
    "\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export a competition-ready metric summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_path = OUTPUT_DIR / 'validation_metrics.json'\n",
    "summary_payload = {\n",
    "    'fold_metrics': fold_df.to_dict(orient='records'),\n",
    "    'aggregated': {\n",
    "        'threshold': median_threshold,\n",
    "        'accuracy': overall_acc,\n",
    "        'precision': overall_precision,\n",
    "        'recall': overall_recall,\n",
    "        'f1': overall_f1,\n",
    "        'jaccard': overall_jaccard,\n",
    "        'auc': overall_auc,\n",
    "    },\n",
    "}\n",
    "with open(summary_path, 'w') as fp:\n",
    "    json.dump(summary_payload, fp, indent=2)\n",
    "print(f'Validation summary stored at {summary_path}')\n",
    "summary_payload\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}