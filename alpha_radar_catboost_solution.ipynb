{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afe35371",
   "metadata": {},
   "source": [
    "# Alpha Radar: Solana Sprint - CatBoost Solution\n",
    "\n",
    "- feature engineers the 30-second PumpFun event stream\n",
    "- trains a CatBoost model while monitoring recall and accuracy\n",
    "- searches thresholds that keep validation accuracy above 90%\n",
    "- exports competition submission plus a positive-token report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    jaccard_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.float_format\", lambda v: f\"{v:0.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ac555",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"Dataset\" / \"alpha-radar-solana-sprint\"\n",
    "TARGET_PATH = BASE_DIR / \"Dataset\" / \"target_tokens.csv\"\n",
    "EVALUATION_PATTERN = \"evaluation_set_30s_chunk_*.csv\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Expected data directory at {DATA_DIR}\")\n",
    "if not TARGET_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Expected target token file at {TARGET_PATH}\")\n",
    "\n",
    "print(f\"Using data directory: {DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b355ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_NUMERIC_COLUMNS = {\"timestamp\", \"mint_token_id\", \"holder\", \"trade_mode\", \"creator\"}\n",
    "\n",
    "def parse_timestamp_series(series: pd.Series) -> pd.Series:\n",
    "    parts = series.astype(str).str.split(\":\", n=1, expand=True)\n",
    "    minutes = pd.to_numeric(parts[0], errors=\"coerce\")\n",
    "    seconds = pd.to_numeric(parts[1], errors=\"coerce\")\n",
    "    return (minutes * 60 + seconds).astype(\"float32\")\n",
    "\n",
    "def load_event_data(csv_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if \"index\" in df.columns:\n",
    "        df = df.drop(columns=[\"index\"])\n",
    "    df[\"timestamp_seconds\"] = parse_timestamp_series(df[\"timestamp\"])\n",
    "    numeric_cols = [c for c in df.columns if c not in NON_NUMERIC_COLUMNS]\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    df[numeric_cols] = df[numeric_cols].astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "def build_features(events: pd.DataFrame) -> pd.DataFrame:\n",
    "    ordered = events.sort_values([\"mint_token_id\", \"timestamp_seconds\"]).reset_index(drop=True)\n",
    "    numeric_cols = ordered.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    grouped = ordered.groupby(\"mint_token_id\", sort=False)\n",
    "    stats = grouped[numeric_cols].agg([\"mean\", \"std\", \"min\", \"max\", \"last\"])\n",
    "    stats.columns = [f\"{col}_{stat}\" for col, stat in stats.columns]\n",
    "    features = stats\n",
    "    features[\"event_count\"] = grouped.size().astype(\"float32\")\n",
    "    features[\"unique_holders\"] = grouped[\"holder\"].nunique().astype(\"float32\")\n",
    "    features[\"unique_creators\"] = grouped[\"creator\"].nunique().astype(\"float32\")\n",
    "    trade_counts = ordered.pivot_table(\n",
    "        index=\"mint_token_id\",\n",
    "        columns=\"trade_mode\",\n",
    "        values=\"timestamp_seconds\",\n",
    "        aggfunc=\"count\",\n",
    "        fill_value=0,\n",
    "    )\n",
    "    trade_counts.columns = [f\"trade_mode_{col}_count\" for col in trade_counts.columns]\n",
    "    features = features.join(trade_counts, how=\"left\")\n",
    "    for col in (\"trade_mode_buy_count\", \"trade_mode_sell_count\"):\n",
    "        if col not in features.columns:\n",
    "            features[col] = 0.0\n",
    "        else:\n",
    "            features[col] = features[col].astype(\"float32\")\n",
    "    event_count = features[\"event_count\"].clip(lower=1.0)\n",
    "    features[\"buy_share\"] = features[\"trade_mode_buy_count\"] / event_count\n",
    "    features[\"sell_share\"] = features[\"trade_mode_sell_count\"] / event_count\n",
    "    features[\"net_buy\"] = features[\"trade_mode_buy_count\"] - features[\"trade_mode_sell_count\"]\n",
    "    if \"timestamp_seconds_last\" in features.columns and \"timestamp_seconds_min\" in features.columns:\n",
    "        features[\"active_duration\"] = features[\"timestamp_seconds_last\"] - features[\"timestamp_seconds_min\"]\n",
    "    if \"sol_volume_last\" in features.columns and \"sol_volume_mean\" in features.columns:\n",
    "        denom = features[\"sol_volume_mean\"].replace(0, np.nan)\n",
    "        features[\"sol_volume_last_to_mean\"] = features[\"sol_volume_last\"] / denom\n",
    "    if \"token_volume_last\" in features.columns and \"token_volume_mean\" in features.columns:\n",
    "        denom = features[\"token_volume_mean\"].replace(0, np.nan)\n",
    "        features[\"token_volume_last_to_mean\"] = features[\"token_volume_last\"] / denom\n",
    "    features = features.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    return features.astype(\"float32\")\n",
    "\n",
    "def evaluate_thresholds(y_true: pd.Series, y_prob: np.ndarray, thresholds: np.ndarray) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"binary\", zero_division=0\n",
    "        )\n",
    "        try:\n",
    "            jac = jaccard_score(y_true, y_pred)\n",
    "        except Exception:\n",
    "            jac = 0.0\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        rows.append(\n",
    "            {\n",
    "                \"threshold\": thr,\n",
    "                \"accuracy\": acc,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"jaccard\": jac,\n",
    "                \"tp\": int(tp),\n",
    "                \"fp\": int(fp),\n",
    "                \"fn\": int(fn),\n",
    "                \"tn\": int(tn),\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_events = load_event_data(DATA_DIR / \"Sample_Dataset.csv\")\n",
    "target_tokens = pd.read_csv(TARGET_PATH, header=None, names=[\"mint_token_id\"])\n",
    "target_set = set(target_tokens[\"mint_token_id\"])\n",
    "\n",
    "print(f\"Sample events shape: {sample_events.shape}\")\n",
    "print(f\"Unique tokens in sample events: {sample_events['mint_token_id'].nunique()}\")\n",
    "print(f\"Target tokens provided: {len(target_set)}\")\n",
    "sample_events.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9003f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = build_features(sample_events)\n",
    "train_features[\"is_target\"] = train_features.index.isin(target_set).astype(\"int8\")\n",
    "\n",
    "print(f\"Training feature matrix: {train_features.shape}\")\n",
    "class_counts = train_features[\"is_target\"].value_counts().rename(\"token_count\")\n",
    "display(class_counts.to_frame())\n",
    "train_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in train_features.columns if col != \"is_target\"]\n",
    "X = train_features[feature_columns]\n",
    "y = train_features[\"is_target\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "model_params = {\n",
    "    \"depth\": 8,\n",
    "    \"learning_rate\": 0.06,\n",
    "    \"iterations\": 1500,\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"eval_metric\": \"AUC\",\n",
    "    \"random_seed\": RANDOM_STATE,\n",
    "    \"scale_pos_weight\": 10.0,\n",
    "    \"l2_leaf_reg\": 5.0,\n",
    "    \"bootstrap_type\": \"Bernoulli\",\n",
    "    \"subsample\": 0.8,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 120,\n",
    "    \"verbose\": 100,\n",
    "}\n",
    "\n",
    "model = CatBoostClassifier(**model_params)\n",
    "model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "val_prob = model.predict_proba(X_val)[:, 1]\n",
    "roc_auc = roc_auc_score(y_val, val_prob)\n",
    "print(f\"Validation ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Best iteration: {model.get_best_iteration()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_pred = (val_prob >= 0.5).astype(int)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_val, default_pred, average=\"binary\", zero_division=0)\n",
    "accuracy = accuracy_score(y_val, default_pred)\n",
    "try:\n",
    "    jaccard = jaccard_score(y_val, default_pred)\n",
    "except Exception:\n",
    "    jaccard = 0.0\n",
    "tn, fp, fn, tp = confusion_matrix(y_val, default_pred).ravel()\n",
    "print(f\"Accuracy @0.50: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}  Recall: {recall:.4f}  F1: {f1:.4f}  Jaccard: {jaccard:.4f}\")\n",
    "print({\"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_grid = np.linspace(0.05, 0.9, 18)\n",
    "threshold_results = evaluate_thresholds(y_val, val_prob, threshold_grid).sort_values(\"threshold\").reset_index(drop=True)\n",
    "display(threshold_results)\n",
    "\n",
    "candidate_mask = (threshold_results[\"accuracy\"] >= 0.90) & (threshold_results[\"recall\"] >= 0.60)\n",
    "if candidate_mask.any():\n",
    "    chosen_row = threshold_results[candidate_mask].sort_values([\"f1\", \"jaccard\"], ascending=False).iloc[0]\n",
    "else:\n",
    "    chosen_row = threshold_results.sort_values([\"recall\", \"accuracy\"], ascending=[False, False]).iloc[0]\n",
    "chosen_threshold = float(chosen_row[\"threshold\"])\n",
    "print(f\"Selected threshold: {chosen_threshold:.2f}\")\n",
    "print(chosen_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_pred = (val_prob >= chosen_threshold).astype(int)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_val, chosen_pred, average=\"binary\", zero_division=0)\n",
    "accuracy = accuracy_score(y_val, chosen_pred)\n",
    "jaccard = jaccard_score(y_val, chosen_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_val, chosen_pred).ravel()\n",
    "print(f\"Validation accuracy @threshold {chosen_threshold:.2f}: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}  Recall: {recall:.4f}  F1: {f1:.4f}  Jaccard: {jaccard:.4f}\")\n",
    "print({\"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5990c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(model.get_feature_importance(), index=feature_columns)\n",
    "top_features = feature_importances.sort_values(ascending=False).head(25)\n",
    "top_features.to_frame(name=\"importance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_files = sorted(DATA_DIR.glob(EVALUATION_PATTERN))\n",
    "if not evaluation_files:\n",
    "    raise FileNotFoundError(\"No evaluation set chunks found.\")\n",
    "\n",
    "eval_events = pd.concat([load_event_data(path) for path in evaluation_files], ignore_index=True)\n",
    "eval_features = build_features(eval_events)\n",
    "eval_features = eval_features.reindex(columns=feature_columns, fill_value=0.0)\n",
    "\n",
    "print(f\"Evaluation events shape: {eval_events.shape}\")\n",
    "print(f\"Evaluation feature matrix: {eval_features.shape}\")\n",
    "eval_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prob = model.predict_proba(eval_features)[:, 1]\n",
    "eval_pred = (eval_prob >= chosen_threshold).astype(int)\n",
    "\n",
    "detailed_df = pd.DataFrame(\n",
    "    {\n",
    "        \"mint_token_id\": eval_features.index,\n",
    "        \"prediction_score\": eval_prob,\n",
    "        \"is_target\": eval_pred,\n",
    "    }\n",
    ").reset_index(drop=True)\n",
    "\n",
    "submission_df = detailed_df[[\"mint_token_id\", \"is_target\"]]\n",
    "positive_df = detailed_df[detailed_df[\"is_target\"] == 1].copy()\n",
    "positive_df[\"threshold_used\"] = chosen_threshold\n",
    "\n",
    "submission_path = BASE_DIR / \"submission.csv\"\n",
    "detailed_path = BASE_DIR / \"predictions_detailed.csv\"\n",
    "positive_path = BASE_DIR / \"predicted_positive_tokens.csv\"\n",
    "\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "detailed_df.to_csv(detailed_path, index=False)\n",
    "positive_df.to_csv(positive_path, index=False)\n",
    "\n",
    "assert submission_df.shape[0] == 64208, \"Submission must contain 64,208 rows.\"\n",
    "\n",
    "print(f\"Saved submission to: {submission_path}\")\n",
    "print(f\"Saved detailed predictions to: {detailed_path}\")\n",
    "print(f\"Saved positive-token report to: {positive_path}\")\n",
    "print(f\"Predicted positives: {positive_df.shape[0]} (threshold={chosen_threshold:.2f})\")\n",
    "submission_df['is_target'].value_counts().to_frame(name='token_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_df.sort_values(\"prediction_score\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95440e",
   "metadata": {},
   "source": [
    "**Next Steps**\n",
    "\n",
    "- Submit submission.csv to the Kaggle competition\n",
    "- Review predicted_positive_tokens.csv for manual due diligence\n",
    "- Iterate on additional feature ideas (e.g. time-window segmentation) to further raise recall without sacrificing accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
